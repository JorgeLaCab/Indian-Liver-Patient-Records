{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RANDOM INITIALIZATION FOR THE PARAMETERS.\n",
    "\n",
    "def randInitLogReg(dim):\n",
    "    # Input:  Size of the vector of weights, dim.\n",
    "    # Output: Vector of Logistic Regression weights w.\n",
    "    #         Scalar of Logistic Regression bias b.\n",
    "    \n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CALCULATES THE LINEAR STEP OF LOGISTIC REGRESSION.\n",
    "\n",
    "def linearLogReg(X, w, b):\n",
    "    # Input:  Matrix of training examples X.\n",
    "    #         Vector of Logistic Regression weights w.\n",
    "    #         Scalar of Logistic Regression bias b.\n",
    "    # Output: Vector containing the result z.\n",
    "    \n",
    "    z = np.dot(w.T,X) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE SIGMOID OF THE VECTOR z.\n",
    "\n",
    "def sigmoidLogReg(z):\n",
    "    # Input:  Vector z.\n",
    "    # Output: Sigmoid of the vector z.\n",
    "    \n",
    "    a = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# THE FUNCTION CALCULATES THE COST FUNCTION OF LOGISTIC REGRESSION.\\n\\ndef costfunctionLogReg(A,X,y):\\n    # Input:  Vector of activation A.\\n    #         Matrix of training examples X.\\n    #         Vector of labels Y.\\n    # Output: Scalar of Logistic Regression cost function result.\\n    \\n    m = np.shape(X)[1]\\n    \\n    cost = -np.sum(np.dot(y,(np.log(A)).T) + np.dot((1-y),(np.log(1-A)).T)) / m\\n    \\n    return cost\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# THE FUNCTION CALCULATES THE COST FUNCTION OF LOGISTIC REGRESSION.\n",
    "\n",
    "def costfunctionLogReg(A,X,y):\n",
    "    # Input:  Vector of activation A.\n",
    "    #         Matrix of training examples X.\n",
    "    #         Vector of labels Y.\n",
    "    # Output: Scalar of Logistic Regression cost function result.\n",
    "    \n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    cost = -np.sum(np.dot(y,(np.log(A)).T) + np.dot((1-y),(np.log(1-A)).T)) / m\n",
    "    \n",
    "    return cost\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CALCULATES THE COST FUNCTION OF LOGISTIC REGRESSION.\n",
    "\n",
    "def costfunctionLogReg(A,X,Y,w,regu,lambd):\n",
    "    # Input:  Vector of activation A.\n",
    "    #         Matrix of training examples X.\n",
    "    #         Vector of labels Y.\n",
    "    #         Vector of Logistic Regression weights w.\n",
    "    #         Variable regu which chooses the regularization method used to calculate the cost function.\n",
    "    #         Hyper-parameter lambda. It's the trade-off between regularization and entropy.\n",
    "    # Output: Scalar of Logistic Regression cost function result.\n",
    "    \n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    cost = -np.sum(np.dot(Y,(np.log(A)).T) + np.dot((1-Y),(np.log(1-A)).T)) / m\n",
    "    \n",
    "    if regu == 1:\n",
    "        cost = cost + (lambd/(2*m))*(np.sum(np.abs(w)))\n",
    "    elif regu == 2:\n",
    "        cost = cost + (lambd/(2*m))*(np.sum(np.multiply(w,w)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# THE FUNCTION COMPUTES THE GRADIENTS OF THE LOGISTIC REGRESSION COST FUNCTION.\\n\\ndef gradientsLogReg(A,X,y):\\n    # Input:  Vector of activation A.\\n    #         Matrix of training examples X.\\n    #         Vector of labels Y.\\n    # Output: Gradient of the cost function with respect to w.\\n    #         Gradient of the cost function with respect to b.\\n    \\n    m = np.shape(X)[1]\\n    \\n    dZ = A - y\\n    dw = (np.dot(X,dZ.T)) / m\\n    db = np.sum(A - y) / m\\n    \\n\\n    return dw, db\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# THE FUNCTION COMPUTES THE GRADIENTS OF THE LOGISTIC REGRESSION COST FUNCTION.\n",
    "\n",
    "def gradientsLogReg(A,X,y):\n",
    "    # Input:  Vector of activation A.\n",
    "    #         Matrix of training examples X.\n",
    "    #         Vector of labels Y.\n",
    "    # Output: Gradient of the cost function with respect to w.\n",
    "    #         Gradient of the cost function with respect to b.\n",
    "    \n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    dZ = A - y\n",
    "    dw = (np.dot(X,dZ.T)) / m\n",
    "    db = np.sum(A - y) / m\n",
    "    \n",
    "\n",
    "    return dw, db\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE GRADIENTS OF THE LOGISTIC REGRESSION COST FUNCTION.\n",
    "\n",
    "def gradientsLogReg(A,X,Y,w,regu,lambd):\n",
    "    # Input:  Vector of activation A.\n",
    "    #         Matrix of training examples X.\n",
    "    #         Vector of labels Y.\n",
    "    #         Vector of Logistic Regression weights w.\n",
    "    #         Variable regu which chooses the regularization method used to calculate the cost function.\n",
    "    #         Hyper-parameter lambda. It's the trade-off between regularization and entropy.\n",
    "    # Output: Gradient of the cost function with respect to w.\n",
    "    #         Gradient of the cost function with respect to b.\n",
    "    \n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    dZ = A - Y\n",
    "    dw = (np.dot(X,dZ.T)) / m\n",
    "    db = np.sum(A - Y) / m\n",
    "    \n",
    "    if regu == 1:\n",
    "        dw = dw + (lambd*np.sign(w)) / m\n",
    "    elif regu == 2:\n",
    "        dw = dw + (lambd*w) / m\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION APPLIES THE GRADIENT DESCENT OPTIMIZATION METHOD.\n",
    "\n",
    "def gradientDescentLogReg(X,y,learningRate,numIterations,regu,lambd):\n",
    "    # Input:  Matrix of training examples X.\n",
    "    #         Vector of labels y.\n",
    "    #         Vector of Logistic Regression weights w.\n",
    "    #         Scalar of Logistic Regression bias b.\n",
    "    #         The learning rate parameter.\n",
    "    #         The number of iterations in gradient descent.\n",
    "    # Output: Vector of Logistic Regression weights w after optimization.\n",
    "    #         Scalar of Logistic Regression bias b after optimization.\n",
    "    #         List of the costs calculated during the optimization process.\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    w, b = randInitLogReg(X.shape[0])\n",
    "    \n",
    "    for i in range(numIterations):\n",
    "        z = linearLogReg(X,w,b)\n",
    "        A = sigmoidLogReg(z)\n",
    "        #cost = costfunctionLogReg(A,X,y)\n",
    "        cost = costfunctionLogReg(A,X,y,w,regu,lambd)\n",
    "        costs.append(cost)\n",
    "        #dw, db = gradientsLogReg(A,X,y)\n",
    "        dw, db = gradientsLogReg(A,X,y,w,regu,lambd)\n",
    "        \n",
    "        w = w - (learningRate*dw)\n",
    "        b = b - (learningRate*db)\n",
    "    \n",
    "    return w, b, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CLASSIFIES THE TEST/VALIDATION EXAMPLES.\n",
    "\n",
    "def classificatorLogReg(X,w,b):\n",
    "    # Input:  Matrix of test/validation examples X.\n",
    "    #         Vector of Logistic Regression weights w.\n",
    "    #         Scalar of Logistic Regression bias b.\n",
    "    # Output: Vector of Logistic Regression labels Y predicted.\n",
    "    \n",
    "    z = linearLogReg(X,w,b)\n",
    "    y_predict = sigmoidLogReg(z)\n",
    "    \n",
    "    y_predict[y_predict>=0.5] = 1\n",
    "    y_predict[y_predict<0.5] = 0\n",
    "    \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION GIVES DIFFERENT EVALUATION METRICS.\n",
    "\n",
    "def evalModelLogReg(y_predicted,y_gt):\n",
    "    # Input:  Vector of Logistic Regression labels Y predicted.\n",
    "    #         Vector of labels Y.\n",
    "    # Output: Precision of the results.\n",
    "    #         Recall of the results.\n",
    "    #         F1 of the results.\n",
    "    #         Accuracy of the results.\n",
    "    \n",
    "    TP = (y_predicted * y_gt == 1).sum()\n",
    "    FP = (y_predicted - y_gt == 1).sum()\n",
    "    TN = (y_predicted + y_gt == 0).sum()\n",
    "    FN = (y_predicted - y_gt == -1).sum()\n",
    "    \n",
    "    Precision = np.round((TP/(TP+FP))*100,decimals=2)\n",
    "    Recall = np.round((TP/(TP+FN))*100,decimals=2)\n",
    "    F1 = np.round(2/((1/Precision)+(1/Recall)),decimals=2)\n",
    "    Accuracy = np.round(((TP+TN)/(TP+TN+FP+FN))*100,decimals=2)\n",
    "    \n",
    "    return Precision, Recall, F1, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# THE FUNCTION CREATES THE MINI-BATCHES OF THE TRAINING EXAMPLES.\\n\\ndef minibatchCreator(X,Y,minibatchSize):\\n    # Input:  Matrix of training examples X.\\n    #         Vector of labels Y.\\n    # Output: Matrix of training examples X randomly shuffled.\\n    #         Vector of labels Y randomly shuffled.\\n    \\n    m = np.shape(X)[1]\\n    \\n    X, Y = randomShuffle(X,Y)\\n    \\n    minibatches = []\\n    minibatchNum = math.floor(m/minibatchSize)\\n    \\n    for t in range(0,minibatchNum):\\n        minibatch_X = X[:,t*(minibatchNum+1):(t+1)*minibatchNum]\\n        minibatch_Y = Y[:,t*(minibatchNum+1):(t+1)*minibatchNum]\\n        mini_batch = (minibatch_X, minibatch_Y)\\n        minibatches.append(mini_batch)\\n    \\n    if m % minibatchSize != 0:\\n        minibatch_X = X[:,(minibatchSize*minibatchNum):]\\n        minibatch_Y = Y[:,(minibatchSize*minibatchNum):]\\n        mini_batch = (minibatch_X, minibatch_Y)\\n        minibatches.append(mini_batch)\\n\\n    return minibatches\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# THE FUNCTION CREATES THE MINI-BATCHES OF THE TRAINING EXAMPLES.\n",
    "\n",
    "def minibatchCreator(X,Y,minibatchSize):\n",
    "    # Input:  Matrix of training examples X.\n",
    "    #         Vector of labels Y.\n",
    "    # Output: Matrix of training examples X randomly shuffled.\n",
    "    #         Vector of labels Y randomly shuffled.\n",
    "    \n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    X, Y = randomShuffle(X,Y)\n",
    "    \n",
    "    minibatches = []\n",
    "    minibatchNum = math.floor(m/minibatchSize)\n",
    "    \n",
    "    for t in range(0,minibatchNum):\n",
    "        minibatch_X = X[:,t*(minibatchNum+1):(t+1)*minibatchNum]\n",
    "        minibatch_Y = Y[:,t*(minibatchNum+1):(t+1)*minibatchNum]\n",
    "        mini_batch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(mini_batch)\n",
    "    \n",
    "    if m % minibatchSize != 0:\n",
    "        minibatch_X = X[:,(minibatchSize*minibatchNum):]\n",
    "        minibatch_Y = Y[:,(minibatchSize*minibatchNum):]\n",
    "        mini_batch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(mini_batch)\n",
    "\n",
    "    return minibatches\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# THE FUNCTION APPLIES THE GRADIENT DESCENT OPTIMIZATION METHOD USING MINI-BATCHES.\\n\\ndef gradientDescentMiniBatch(X,Y,learningRate,numEpochs,minibatchSize):\\n    # Input:  Matrix of training examples X.\\n    #         Vector of labels Y.\\n    #         Vector of Logistic Regression weights w.\\n    #         Scalar of Logistic Regression bias b.\\n    #         The learning rate parameter.\\n    #         The number of times the whole training set is analized.\\n    #         Size of the mini-batch.\\n    # Output: Vector of Logistic Regression weights w after optimization.\\n    #         Scalar of Logistic Regression bias b after optimization.\\n    #         List of the costs calculated during the optimization process.\\n    \\n    costs = []\\n    \\n    w, b = randInit(X.shape[0])\\n    \\n    for i in range(numEpochs):\\n        \\n        minibatches = minibatchCreator(X,Y,minibatchSize)\\n        \\n        for minibatch in minibatches:\\n            (minibatch_X,minibatch_Y) = minibatch\\n            \\n            z = lineal(minibatch_X,w,b)\\n            A = sigmoid(z)\\n            cost = costfunctionLogReg(A,minibatch_X,minibatch_Y)\\n            dw, db = gradientsLogReg(A,minibatch_X,minibatch_Y)\\n            \\n            w = w - (learningRate*dw)\\n            b = b - (learningRate*db)\\n        \\n        costs.append(cost)\\n    \\n    return w, b, costs\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# THE FUNCTION APPLIES THE GRADIENT DESCENT OPTIMIZATION METHOD USING MINI-BATCHES.\n",
    "\n",
    "def gradientDescentMiniBatch(X,Y,learningRate,numEpochs,minibatchSize):\n",
    "    # Input:  Matrix of training examples X.\n",
    "    #         Vector of labels Y.\n",
    "    #         Vector of Logistic Regression weights w.\n",
    "    #         Scalar of Logistic Regression bias b.\n",
    "    #         The learning rate parameter.\n",
    "    #         The number of times the whole training set is analized.\n",
    "    #         Size of the mini-batch.\n",
    "    # Output: Vector of Logistic Regression weights w after optimization.\n",
    "    #         Scalar of Logistic Regression bias b after optimization.\n",
    "    #         List of the costs calculated during the optimization process.\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    w, b = randInit(X.shape[0])\n",
    "    \n",
    "    for i in range(numEpochs):\n",
    "        \n",
    "        minibatches = minibatchCreator(X,Y,minibatchSize)\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X,minibatch_Y) = minibatch\n",
    "            \n",
    "            z = lineal(minibatch_X,w,b)\n",
    "            A = sigmoid(z)\n",
    "            cost = costfunctionLogReg(A,minibatch_X,minibatch_Y)\n",
    "            dw, db = gradientsLogReg(A,minibatch_X,minibatch_Y)\n",
    "            \n",
    "            w = w - (learningRate*dw)\n",
    "            b = b - (learningRate*db)\n",
    "        \n",
    "        costs.append(cost)\n",
    "    \n",
    "    return w, b, costs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
