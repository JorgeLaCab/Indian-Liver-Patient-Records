{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RANDOM INITIALIZATION FOR THE PARAMETERS.\n",
    "\n",
    "def randInitNN(dimLayers):\n",
    "    # Input:  Size of the vector of weights for each layer of the NN.\n",
    "    # Output: List of parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    \n",
    "    parameters = []\n",
    "    \n",
    "    for l in range(1,len(dimLayers)):\n",
    "        W = np.random.randn(dimLayers[l],dimLayers[l-1]) / np.sqrt(dimLayers[l-1])\n",
    "        b = np.zeros([dimLayers[l],1])\n",
    "        \n",
    "        parameters.append({\"Weights\": W,\"bias\": b})\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CALCULATES THE LINEAR STEP FOR A NEURONAL NETWORK LAYER.\n",
    "\n",
    "def linearNN(X,W,b):\n",
    "    # Input:  Matrix of examples X.\n",
    "    #         Matrix of weights W.\n",
    "    #         Vector of bias b.\n",
    "    # Output: Matrix Z, output of the linear layer.\n",
    "    \n",
    "    Z = np.dot(W,X) + b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE SIGMOID OF THE MATRIX Z, OUTPUT OF THE LINEAR STEP.\n",
    "\n",
    "def sigmoidForeward(Z):\n",
    "    # Input:  Matrix Z, output of the linear step.\n",
    "    # Output: Post-activation matrix A.\n",
    "    \n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE RELU OF THE MATRIX Z, OUTPUT OF THE LINEAR STEP.\n",
    "\n",
    "def reluForeward(Z):\n",
    "    # Input:  Matrix Z, output of the linear step.\n",
    "    # Output: Post-activation matrix A.\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CALCULATES THE FOREWARD MODEL OF A NEURAL NETWORK.\n",
    "\n",
    "def forewardProp(X,parameters):\n",
    "    # Input:  Matrix of examples X.\n",
    "    #         List of parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    # Output: List which contains the matrix Z output of the linear layer and the post-activation matrix A for each layer.\n",
    "    \n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    \n",
    "    for i in range(len(parameters)-1):\n",
    "        W = parameters[i][\"Weights\"]\n",
    "        b = parameters[i][\"bias\"]\n",
    "        \n",
    "        Z = linearNN(A, W, b)\n",
    "        A = reluForeward(Z)\n",
    "        \n",
    "        caches.append({\"Z\": Z,\"A\": A})\n",
    "\n",
    "    WL = parameters[len(parameters)-1][\"Weights\"]\n",
    "    bL = parameters[len(parameters)-1][\"bias\"]\n",
    "    ZL = linearNN(A,WL,bL)\n",
    "    AL = sigmoidForeward(ZL)\n",
    "    \n",
    "    caches.append({\"Z\": ZL,\"A\": AL})\n",
    "    \n",
    "    return caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CALCULATES THE COST FUNCTION OF A NEURAL NETWORK.\n",
    "\n",
    "def costfunctionNN(AL,Y,parameters,regu,lambd):\n",
    "    # Input:  Vector of activation A of the last layer L.\n",
    "    #         Vector of labels Y.\n",
    "    #         List of parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    #         Variable regu which chooses the regularization method used to calculate the cost function.\n",
    "    #         Hyper-parameter lambda. It's the trade-off between regularization and entropy.\n",
    "    # Output: Scalar of Neural Network cost function result.\n",
    "    \n",
    "    m = np.shape(Y)[1]\n",
    "    \n",
    "    #cost = -np.sum(np.dot(Y,(np.log(AL)).T) + np.dot((1-Y),(np.log(1-AL)).T)) * (1./m)\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y,np.log(1-AL).T))\n",
    "    \n",
    "    if regu == 1:\n",
    "        costL1 = 0\n",
    "        for i in range(len(parameters)):\n",
    "            W = parameters[i][\"Weights\"]\n",
    "            costL1 = costL1 + np.sum(np.abs(W))\n",
    "        cost = cost + (lambd/(2*m))*costL1\n",
    "    elif regu == 2:\n",
    "        costL2 = 0\n",
    "        for i in range(len(parameters)):\n",
    "            W = parameters[i][\"Weights\"]\n",
    "            costL2 = costL2 + np.sum(np.square(W))\n",
    "        cost = cost + (lambd/(2*m))*costL2\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE DERIVATIVE OF THE COST FUNCTION WITH RESPECT TO THE MATRIX Z WHEN USING SIGMOID FUNCTION.\n",
    "\n",
    "def sigmoidBackward(dA,Z):\n",
    "    # Input:  Post-activation gradient dA.\n",
    "    #         Matrix Z, output of the linear step.\n",
    "    # Output: Gradient of the cost with respect to Z.\n",
    "    \n",
    "    S = 1/(1+np.exp(-Z))\n",
    "    dZ = dA*S*(1-S)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE DERIVATIVE OF THE COST FUNCTION WITH RESPECT TO THE MATRIX Z WHEN USING RELU FUNCTION.\n",
    "\n",
    "def reluBackward(dA,Z):\n",
    "    # Input:  Post-activation gradient dA.\n",
    "    #         Matrix Z, output of the linear step.\n",
    "    # Output: Gradient of the cost with respect to Z.\n",
    "\n",
    "    dZ = np.array(dA,copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z<=0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION COMPUTES THE DERIVATIVE OF THE COST FUNCTION WITH RESPECT TO THE PARAMETERS W AND b.\n",
    "\n",
    "def gradientsNN(dZ,A_prev,m,W,regu,lambd):\n",
    "    # Input:  Gradient of the cost with respect to Z.\n",
    "    #         Vector of activation A of the previous layer.\n",
    "    #         Number of examples.\n",
    "    #         Vector of weights W.\n",
    "    #         Variable regu which chooses the regularization method used to calculate the cost function.\n",
    "    #         Hyper-parameter lambda. It's the trade-off between regularization and entropy.\n",
    "    # Output: Gradient of the cost with respect to the weights matrix W.\n",
    "    #         Gradient of the cost with respect to the bias vector b.\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T) / m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True) / m\n",
    "    \n",
    "    if regu == 1:\n",
    "        dW = dW + (lambd*np.sign(W)) / m\n",
    "    elif regu == 2:\n",
    "        dW = dW + (lambd*W) / m\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CALCULATES THE BACKWARD MODEL OF A NEURAL NETWORK.\n",
    "\n",
    "def backwardProp(X,Y,parameters,caches,regu,lambd):\n",
    "    # Input:  Matrix of examples X.\n",
    "    #         Vector of labels Y.\n",
    "    #         List of parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    #         List which contains the matrix Z output of the linear layer and the post-activation matrix A for each layer.\n",
    "    #         Variable regu which chooses the regularization method used to calculate the cost function.\n",
    "    #         Hyper-parameter lambda. It's the trade-off between regularization and entropy.\n",
    "    # Output: List which contains the derivative of the cost function with respect the parameters W and b for each layer.\n",
    "    \n",
    "    gradients = []\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    AL = caches[len(parameters)-1][\"A\"]\n",
    "    ZL = caches[len(parameters)-1][\"Z\"]\n",
    "    A_prev = caches[len(parameters)-2][\"A\"]\n",
    "    \n",
    "    dAL = - (np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
    "    dZL = sigmoidBackward(dAL,ZL)\n",
    "    W = parameters[len(parameters)-1][\"Weights\"]\n",
    "    dWL, dbL = gradientsNN(dZL,A_prev,m,W,regu,lambd)\n",
    "    \n",
    "    gradients.append({\"dW\": dWL,\"db\": dbL})\n",
    "    \n",
    "    dZ = dZL\n",
    "    \n",
    "    for i in reversed(range(len(parameters)-1)):\n",
    "        if i == 0:\n",
    "            A_prev = X\n",
    "        else:\n",
    "            A_prev = caches[i-1][\"A\"]\n",
    "            \n",
    "        Z = caches[i][\"Z\"]\n",
    "        W = parameters[i+1][\"Weights\"]       \n",
    "        dA = np.dot(W.T,dZ)\n",
    "        dZ = reluBackward(dA,Z)\n",
    "        \n",
    "        W = parameters[i][\"Weights\"]\n",
    "        dW, db = gradientsNN(dZ,A_prev,m,W,regu,lambd)\n",
    "        \n",
    "        gradients.append({\"dW\": dW,\"db\": db})\n",
    "    \n",
    "    gradients.reverse()\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION UPDATES THE PARAMETERS.\n",
    "\n",
    "def updateParametersNN(parameters,gradients,learningRate):\n",
    "    # Input:  List of parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    #         List which contains the derivative of the cost function with respect the parameters W and b for each layer.\n",
    "    #         Learning rate hyper-parameter.\n",
    "    # Output: List of updated parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    \n",
    "    for i in range(len(parameters)):\n",
    "        parameters[i][\"Weights\"] = parameters[i][\"Weights\"] - learningRate*gradients[i][\"dW\"]\n",
    "        parameters[i][\"bias\"] = parameters[i][\"bias\"] - learningRate*gradients[i][\"db\"]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CREATES A NEURAL NETWORK MODEL.\n",
    "\n",
    "def NNmodel(X,Y,dimLayers,learningRate,numIterations,regu=0,lambd=0):\n",
    "    # Input:  Matrix of examples X.\n",
    "    #         Vector of labels Y.\n",
    "    #         Size of the vector of weights for each layer of the NN.\n",
    "    #         Learning rate hyper-parameter.\n",
    "    #         Number of iterations.\n",
    "    #         Variable regu which chooses the regularization method used to calculate the cost function.\n",
    "    #         Hyper-parameter lambda. It's the trade-off between regularization and entropy.\n",
    "    # Output: List of parameters of the Neural Network, the matrix of weights W and vector of bias b for each layer.\n",
    "    #         List of scalars of Neural Network cost function results for each iteration.\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    parameters = randInitNN(dimLayers)\n",
    "    \n",
    "    for i in range(0,numIterations):\n",
    "        \n",
    "        caches = forewardProp(X,parameters)\n",
    "        \n",
    "        AL = caches[len(parameters)-1][\"A\"]\n",
    "\n",
    "        cost = costfunctionNN(AL,Y,parameters,regu,lambd)\n",
    "        \n",
    "        gradients = backwardProp(X,Y,parameters,caches,regu,lambd)\n",
    " \n",
    "        parameters = updateParametersNN(parameters,gradients,learningRate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        #if i % 100 == 0:\n",
    "        #    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        #if i % 100 == 0:\n",
    "        #    costs.append(cost)\n",
    "        costs.append(cost)\n",
    "        \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION CLASSIFIES THE TEST/VALIDATION EXAMPLES.\n",
    "\n",
    "def classificatorNN(X,parameters):\n",
    "    # Input:  Matrix of test/validation examples X.\n",
    "    #         List of parameters of the NN: Matrix of weights W and vector of bias b for each layer.\n",
    "    # Output: Vector of Neural Networks labels Y predicted.\n",
    "    \n",
    "    caches = forewardProp(X,parameters)\n",
    "    Y_predict = caches[len(parameters)-1][\"A\"]\n",
    "    \n",
    "    Y_predict[Y_predict>=0.5] = 1\n",
    "    Y_predict[Y_predict<0.5] = 0\n",
    "    \n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE FUNCTION GIVES DIFFERENT EVALUATION METRICS.\n",
    "\n",
    "def evalModelNN(y_predicted,y_gt):\n",
    "    # Input:  Vector of Neural Networks labels y predicted.\n",
    "    #         Vector of labels y.\n",
    "    # Output: Precision of the results.\n",
    "    #         Recall of the results.\n",
    "    #         F1 of the results.\n",
    "    #         Accuracy of the results.\n",
    "    \n",
    "    TP = (y_predicted * y_gt == 1).sum()\n",
    "    FP = (y_predicted - y_gt == 1).sum()\n",
    "    TN = (y_predicted + y_gt == 0).sum()\n",
    "    FN = (y_predicted - y_gt == -1).sum()\n",
    "    \n",
    "    Precision = np.round((TP/(TP+FP))*100,decimals=2)\n",
    "    Recall = np.round((TP/(TP+FN))*100,decimals=2)\n",
    "    F1 = np.round(2/((1/Precision)+(1/Recall)),decimals=2)\n",
    "    Accuracy = np.round(((TP+TN)/(TP+TN+FP+FN))*100,decimals=2)\n",
    "    \n",
    "    return Precision, Recall, F1, Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
